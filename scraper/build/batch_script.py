# code generated by Claude https://claude.ai/chat/9b0de591-0e95-4c91-8e9a-61f233c81716

import os
import requests
import zipfile
import numpy as np
import pandas as pd
import geopandas as gpd
import rasterio
from rasterio.features import shapes
from pyproj import CRS
import boto3
from datetime import datetime
import tempfile
import pytz

def download_file(url, local_filename):
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        with open(local_filename, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192): 
                f.write(chunk)
    print(f"Downloaded {local_filename}")

def geotiff_to_geodataframe(file_path):
    with rasterio.open(file_path) as src:
        image = src.read(1)
        mask = image != src.nodata
        results = (
            {'properties': {'raster_value': v}, 'geometry': s}
            for i, (s, v) 
            in enumerate(shapes(image, mask=mask, transform=src.transform))
        )
        geoms = list(results)
    
    gdf = gpd.GeoDataFrame.from_features(geoms, crs=src.crs)
    return gdf.to_crs(epsg=4326)

def convert_excel_to_parquet(excel_file_path, parquet_file_path):
    df = pd.read_excel(excel_file_path)
    df.to_parquet(parquet_file_path, index=False)
    print(f"Converted {excel_file_path} to {parquet_file_path}")

def load_heat_risk_data(data_dir):
    url_list = {
        "Day 1": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_1_Mercator.tif",
        "Day 2": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_2_Mercator.tif",
        "Day 3": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_3_Mercator.tif",
        "Day 4": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_4_Mercator.tif",
        "Day 5": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_5_Mercator.tif",
        "Day 6": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_6_Mercator.tif",
        "Day 7": "https://www.wpc.ncep.noaa.gov/heatrisk/data/HeatRisk_7_Mercator.tif"
    }
    
    geodataframes = {}
    for day, url in url_list.items():
        file_path = os.path.join(data_dir, f"{day}.tif")
        download_file(url, file_path)
        gdf = geotiff_to_geodataframe(file_path)
        geodataframes[day] = gdf
    
    return geodataframes

def load_cdc_data(data_dir):
    boundaries_url = "https://www2.census.gov/geo/tiger/GENZ2020/shp/cb_2020_us_zcta520_500k.zip"
    boundaries_zip = os.path.join(data_dir, "cb_2020_us_zcta520_500k.zip")
    download_file(boundaries_url, boundaries_zip)
    with zipfile.ZipFile(boundaries_zip, 'r') as zip_ref:
        zip_ref.extractall(data_dir)
    boundaries_file = os.path.join(data_dir, "cb_2020_us_zcta520_500k.shp")
    zcta_boundaries = gpd.read_file(boundaries_file).to_crs(epsg=4326)
    
    excel_url = "https://gis.cdc.gov/HHI/Documents/HHI_Data.zip"
    excel_zip = os.path.join(data_dir, "HHI_Data.zip")
    excel_file = os.path.join(data_dir, "HHI Data 2024 United States.xlsx")
    parquet_file = os.path.join(data_dir, "HHI Data 2024 United States.parquet")
    
    download_file(excel_url, excel_zip)
    with zipfile.ZipFile(excel_zip, 'r') as zip_ref:
        zip_ref.extractall(data_dir)
    convert_excel_to_parquet(excel_file, parquet_file)
    hhi_data = pd.read_parquet(parquet_file)
    
    hhi_data['ZCTA'] = hhi_data['ZCTA'].astype(str)
    zcta_boundaries['ZCTA5CE20'] = zcta_boundaries['ZCTA5CE20'].astype(str)
    zcta_with_hhi = zcta_boundaries.merge(hhi_data, left_on='ZCTA5CE20', right_on='ZCTA', how='inner')
    
    return zcta_with_hhi

def create_combined_map(layer1, layer2):
    if layer1.crs != layer2.crs:
        layer2 = layer2.to_crs(layer1.crs)

    if layer1.crs.is_geographic:
        utm_crs = CRS.from_epsg(5070)
        layer1 = layer1.to_crs(utm_crs)
        layer2 = layer2.to_crs(utm_crs)

    joined = gpd.overlay(layer1, layer2, how='intersection')

    joined['intersection_area'] = joined.geometry.area
    joined['weight'] = joined['intersection_area'] / joined.groupby(level=0)['intersection_area'].transform('sum')

    numeric_columns = layer2.select_dtypes(include=[np.number]).columns.drop('geometry', errors='ignore')
    non_numeric_columns = layer2.columns.drop(numeric_columns).drop('geometry', errors='ignore')

    for col in numeric_columns:
        joined[f'weighted_{col}'] = joined[col] * joined['weight']

    for col in non_numeric_columns:
        joined[f'mode_{col}'] = joined.groupby(level=0).apply(
            lambda x: pd.Series({
                col: x.loc[x['intersection_area'].idxmax(), col]
            })
        )

    numeric_result = joined.groupby(level=0).agg({f'weighted_{col}': 'sum' for col in numeric_columns})
    non_numeric_result = joined.groupby(level=0).agg({f'mode_{col}': 'first' for col in non_numeric_columns})

    result = pd.concat([numeric_result, non_numeric_result], axis=1)

    layer1_with_weighted_values = layer1.join(result)

    if not layer1_with_weighted_values.crs.is_geographic:
        layer1_with_weighted_values = layer1_with_weighted_values.to_crs(epsg=4326)

    return layer1_with_weighted_values

def save_to_s3(data, bucket_name, file_name):
    s3 = boto3.client('s3')
    with tempfile.NamedTemporaryFile(delete=False, suffix='.geoparquet') as temp_file:
        temp_file_path = temp_file.name
        data.to_parquet(temp_file_path)
    
    try:
        s3.upload_file(temp_file_path, bucket_name, file_name)
        print(f"Saved {file_name} to S3 bucket {bucket_name}")
    finally:
        os.remove(temp_file_path)

def main():
    with tempfile.TemporaryDirectory() as data_dir:
        geodataframes = load_heat_risk_data(data_dir)
        zcta_with_hhi = load_cdc_data(data_dir)
        bucket_name = os.environ['BUCKET_NAME']
        
        # Get the current time in New York
        ny_tz = pytz.timezone('America/New_York')
        ny_time = datetime.now(ny_tz)
        datestamp = ny_time.strftime("%Y%m%d")

        for day, gdf in geodataframes.items():
            result = create_combined_map(gdf, zcta_with_hhi)
            file_name = f"heat_risk_analysis_{day}_{datestamp}.geoparquet"
            save_to_s3(result, bucket_name, file_name)
            print(f"Processed and saved {file_name}")

if __name__ == "__main__":
    main()